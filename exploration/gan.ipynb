{"cells":[{"cell_type":"markdown","source":["# Set up steps\n","#### Can be ignored and skip to \"Start of Model\" section"],"metadata":{"id":"69WWszk6vqaf"},"id":"69WWszk6vqaf"},{"cell_type":"code","execution_count":null,"id":"af6ae2d8","metadata":{"id":"af6ae2d8"},"outputs":[],"source":["import cv2\n","import csv\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import glob\n","from scipy import stats\n","from typing import List, Tuple\n","from tqdm import tqdm\n","from tensorflow import keras\n","from tensorflow.keras import layers, models, optimizers\n","from tensorflow.keras.models import load_model\n","import time, math\n","from scipy.ndimage import binary_dilation\n","\n","Image = np.ndarray\n","CharacterWithLabel = tuple[Image, str]"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)\n","%ls\n","%cd .."],"metadata":{"id":"2wHNbth7Oo7H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763041395818,"user_tz":-480,"elapsed":4329,"user":{"displayName":"Bryan Ho","userId":"17111691639907127460"}},"outputId":"cd7ac799-6e95-4af2-a160-85233421845a"},"id":"2wHNbth7Oo7H","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","\u001b[0m\u001b[01;36mbin\u001b[0m@                        \u001b[01;36mlib32\u001b[0m@                    \u001b[01;34mroot\u001b[0m/\n","\u001b[01;34mboot\u001b[0m/                       \u001b[01;36mlib64\u001b[0m@                    \u001b[01;34mrun\u001b[0m/\n","\u001b[01;34mcontent\u001b[0m/                    \u001b[01;36mlibx32\u001b[0m@                   \u001b[01;36msbin\u001b[0m@\n","cuda-keyring_1.1-1_all.deb  \u001b[01;34mmedia\u001b[0m/                    \u001b[01;34msrv\u001b[0m/\n","\u001b[01;34mdatalab\u001b[0m/                    \u001b[01;34mmnt\u001b[0m/                      \u001b[01;34msys\u001b[0m/\n","\u001b[01;34mdev\u001b[0m/                        NGC-DL-CONTAINER-LICENSE  \u001b[30;42mtmp\u001b[0m/\n","\u001b[01;34metc\u001b[0m/                        \u001b[01;34mopt\u001b[0m/                      \u001b[01;34mtools\u001b[0m/\n","\u001b[01;34mhome\u001b[0m/                       \u001b[01;34mproc\u001b[0m/                     \u001b[01;34musr\u001b[0m/\n","\u001b[01;34mkaggle\u001b[0m/                     \u001b[01;34mpython-apt\u001b[0m/               \u001b[01;34mvar\u001b[0m/\n","\u001b[01;36mlib\u001b[0m@                        \u001b[01;32mpython-apt.tar.xz\u001b[0m*\n","/\n"]}]},{"cell_type":"code","source":["# sanity check that we can access the necessary contents from drive\n","SOURCE_DIR = '/content/drive/MyDrive/cs4243-project/preprocessing/clean/char_normalised/combined_images_original.npy'\n","\n","if os.path.exists(SOURCE_DIR):\n","    print(f\"The folder exists.\")\n","else:\n","    print(f\"The folder does not exist.\")"],"metadata":{"id":"BlxDo8dpP_p5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763041395830,"user_tz":-480,"elapsed":10,"user":{"displayName":"Bryan Ho","userId":"17111691639907127460"}},"outputId":"c9cc0d9f-5d9a-475b-f69f-9bd972b4dd07"},"id":"BlxDo8dpP_p5","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The folder exists.\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","print(\"Found GPU at: {}\".format(tf.test.gpu_device_name()))"],"metadata":{"id":"ILcxVyQlpFBS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763041399941,"user_tz":-480,"elapsed":4,"user":{"displayName":"Bryan Ho","userId":"17111691639907127460"}},"outputId":"0bfe8656-fcb5-4918-8ab2-0e6d177ce9ee"},"id":"ILcxVyQlpFBS","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Num GPUs Available:  1\n","Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    print(\"GPU is available:\", gpus)\n","    try:\n","        # Set TensorFlow to use only the first GPU\n","        tf.config.set_visible_devices(gpus[0], 'GPU')\n","        logical_gpus = tf.config.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n","    except RuntimeError as e:\n","        # Visible devices must be set before GPUs have been initialized\n","        print(e)\n","else:\n","    print(\"No GPU devices found.\")"],"metadata":{"id":"apcEfQK5ewHl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763041400701,"user_tz":-480,"elapsed":7,"user":{"displayName":"Bryan Ho","userId":"17111691639907127460"}},"outputId":"2fcfb47e-b54a-49b8-a07d-0150546b20d4"},"id":"apcEfQK5ewHl","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","1 Physical GPUs, 1 Logical GPU\n"]}]},{"cell_type":"markdown","source":["# Start of Model\n","### Defining functions, models and model parameters"],"metadata":{"id":"qhF3MhEjMy2y"},"id":"qhF3MhEjMy2y"},{"cell_type":"code","source":["# =========================\n","# 0) Globals\n","# =========================\n","\n","# Image / model params\n","IMG_SHAPE   = (80, 80, 1)\n","H, W, C     = IMG_SHAPE\n","LATENT_DIM  = 100\n","BATCH_SIZE  = 64\n","N_CRITIC    = 5          # D steps per G step\n","GP_WEIGHT   = 10.0\n","G_LR        = 1e-4     # TTUR (slower G)\n","D_LR        = 4e-4     # TTUR (faster D)\n","\n","# =========================\n","# 1) Data loading\n","# =========================\n","def load_char_data(csv_path, npy_path):\n","    \"\"\"Load images (.npy) + labels (.csv), convert to grayscale (1 channel), normalize to [-1,1].\"\"\"\n","    imgs = np.load(npy_path)                       # Could be (N, H, W) or (N, H, W, C_initial)\n","\n","    labels = []\n","    with open(csv_path) as f:\n","        for i, row in enumerate(csv.reader(f)):\n","            labels.append(row[0])\n","    if labels:\n","        labels = labels[1:]                        # drop header if present\n","\n","    # Ensure images have 1 channel (grayscale) and shape (N, H, W, 1)\n","    if len(imgs.shape) == 3: # (N, H, W) - assume grayscale, add channel\n","        imgs = np.expand_dims(imgs, axis=-1) # -> (N, H, W, 1)\n","    elif len(imgs.shape) == 4:\n","        if imgs.shape[-1] == 3: # (N, H, W, 3) - assume RGB, convert to grayscale\n","            imgs = np.mean(imgs, axis=-1, keepdims=True) # Average to grayscale, keep channel dim\n","        elif imgs.shape[-1] == 1: # (N, H, W, 1) - already grayscale\n","            pass\n","        else:\n","            raise ValueError(f\"Unexpected number of channels in image data: {imgs.shape}. Expected 1 or 3.\")\n","    else:\n","        # This handles cases like (N, H, W, 3, 1) if it came from previous faulty execution\n","        # or other unexpected dimensions. We want to convert it to (N, H, W, 1).\n","        print(f\"Warning: Image data loaded with unexpected shape {imgs.shape}. Attempting to convert to (N, H, W, 1).\")\n","        imgs = np.squeeze(imgs) # Remove all dimensions of size 1\n","        if len(imgs.shape) == 3: # After squeeze, if it's (N, H, W) or (N, H, W, C_final)\n","            if imgs.shape[-1] == 3: # If 3 channels after squeeze\n","                imgs = np.mean(imgs, axis=-1, keepdims=True)\n","            else: # Assume (N, H, W), add channel\n","                imgs = np.expand_dims(imgs, axis=-1)\n","        elif len(imgs.shape) == 4 and imgs.shape[-1] == 1: # If it became (N, H, W, 1) after squeeze\n","             pass\n","        else:\n","            raise ValueError(f\"Failed to convert image data to (N, H, W, 1) from {imgs.shape} after squeeze.\")\n","\n","\n","    imgs = imgs.astype(\"float32\")\n","    # Normalize to [-1,1] for tanh generator\n","    imgs = (imgs / 127.5) - 1.0\n","\n","    print(f\"[load] X shape={imgs.shape} min={imgs.min():.3f} max={imgs.max():.3f} mean={imgs.mean():.3f} std={imgs.std():.3f}\")\n","    return imgs, labels\n","\n","def make_dataset(X, batch_size=BATCH_SIZE, shuffle=8192):\n","    \"\"\"Create endless shuffled tf.data pipeline.\"\"\"\n","    ds = tf.data.Dataset.from_tensor_slices(X)\n","    ds = ds.shuffle(shuffle).repeat().batch(batch_size, drop_remainder=True)\n","    return iter(ds)\n","\n","# =========================\n","# 2) C-GAN compositing utils\n","# =========================\n","def random_blob_mask(h, w, blobs=3, p=0.45, dilate=2):\n","    mask = np.zeros((h, w), dtype=bool)\n","    for _ in range(blobs):\n","        seeds = np.random.rand(h, w) < p\n","        if dilate > 0:\n","            seeds = binary_dilation(seeds, iterations=np.random.randint(1, dilate + 1))\n","        mask = np.logical_or(mask, seeds)\n","    return mask.astype(np.float32)\n","\n","def compose_two(img1, img2, mode=\"mask\", alpha_range=(0.35, 0.65)):\n","    \"\"\"img1/img2 in [-1,1], shape (H,W,1).\"\"\"\n","    if mode == \"alpha\":\n","        a = np.random.uniform(*alpha_range)\n","        return np.clip(a*img1 + (1.0-a)*img2, -1.0, 1.0)\n","    m = random_blob_mask(H, W, blobs=np.random.randint(2,5),\n","                         p=np.random.uniform(0.25,0.5),\n","                         dilate=np.random.randint(1,4))\n","    m = m[..., None]\n","    # mask operates in [-1,1] space\n","    return np.clip(m*img1 + (1.0-m)*img2, -1.0, 1.0)\n","\n","def composite_batch(X, batch_size, k=2, mode=\"mask\", keep_plain_prob=0.15):\n","    idxs = np.random.randint(0, X.shape[0], size=(batch_size, k))\n","    out = np.empty((batch_size, H, W, C), dtype=np.float32)\n","    for i in range(batch_size):\n","        if np.random.rand() < keep_plain_prob:\n","            out[i] = X[idxs[i,0]]\n","            continue\n","        img = X[idxs[i,0]]\n","        for j in range(1, k):\n","            img = compose_two(img, X[idxs[i,j]], mode=mode)\n","        out[i] = img\n","    return out\n","\n","# =========================\n","# 3) Models\n","# =========================\n","# creates the images\n","def build_generator():\n","    return models.Sequential([\n","        layers.Input(shape=(LATENT_DIM,)),\n","        layers.Dense(10*10*256, use_bias=False),\n","        layers.BatchNormalization(), layers.LeakyReLU(),\n","        layers.Reshape((10,10,256)),\n","        layers.Conv2DTranspose(128, 5, strides=2, padding='same', use_bias=False),\n","        layers.BatchNormalization(), layers.LeakyReLU(),\n","        layers.Conv2DTranspose(64, 5, strides=2, padding='same', use_bias=False),\n","        layers.BatchNormalization(), layers.LeakyReLU(),\n","        layers.Conv2DTranspose(1, 5, strides=2, padding='same', activation='tanh', use_bias=False)\n","    ], name=\"generator\")\n","\n","# scores images and learns a function whose output difference approximates\n","# the Wasserstein distance between real and fake.\n","def build_critic():\n","    return models.Sequential([\n","        layers.Input(shape=IMG_SHAPE),\n","        layers.Conv2D(64, 5, strides=2, padding='same'), layers.LeakyReLU(0.2),\n","        layers.Conv2D(128, 5, strides=2, padding='same'), layers.LeakyReLU(0.2),\n","        layers.Flatten(),\n","        layers.Dense(1)  # no activation (critic score)\n","    ], name=\"critic\")\n","\n","# =========================\n","# 4) WGAN-GP training steps\n","# =========================\n","def interpolate(a, b):\n","    alpha = tf.random.uniform([tf.shape(a)[0], 1, 1, 1], 0., 1.)\n","    return a + alpha * (b - a)\n","\n","def gradient_penalty(critic, real, fake):\n","    mixed = interpolate(real, fake)\n","    with tf.GradientTape() as gp_tape:\n","        gp_tape.watch(mixed)\n","        pred = critic(mixed, training=True)\n","    grads = gp_tape.gradient(pred, [mixed])[0]\n","    grads = tf.reshape(grads, [tf.shape(grads)[0], -1])\n","    gp = tf.reduce_mean((tf.norm(grads, axis=1) - 1.0) ** 2)\n","    return gp\n","\n","@tf.function\n","def d_train_step(generator, critic, d_opt, real_batch):\n","    z = tf.random.normal([tf.shape(real_batch)[0], LATENT_DIM])\n","    fake_imgs = generator(z, training=True)\n","    with tf.GradientTape() as tape:\n","        real_logits = critic(real_batch, training=True)\n","        fake_logits = critic(fake_imgs, training=True)\n","        w_dist = tf.reduce_mean(fake_logits) - tf.reduce_mean(real_logits)\n","        gp = gradient_penalty(critic, real_batch, fake_imgs) * GP_WEIGHT\n","        d_loss = w_dist + gp\n","    grads = tape.gradient(d_loss, critic.trainable_variables)\n","    d_opt.apply_gradients(zip(grads, critic.trainable_variables))\n","    return d_loss, w_dist, gp\n","\n","@tf.function\n","def g_train_step(generator, critic, g_opt, batch_size):\n","    z = tf.random.normal([batch_size, LATENT_DIM])\n","    with tf.GradientTape() as tape:\n","        fake_imgs = generator(z, training=True)\n","        fake_logits = critic(fake_imgs, training=True)\n","        g_loss = -tf.reduce_mean(fake_logits)\n","    grads = tape.gradient(g_loss, generator.trainable_variables)\n","    g_opt.apply_gradients(zip(grads, generator.trainable_variables))\n","    return g_loss\n","\n","# =========================\n","# 5) Trainer\n","# =========================\n","def train(\n","    X, steps=20000, outdir=None,\n","    use_composites=False, k_comp=2, mode=\"mask\",\n","    warmup_steps=5000, keep_plain_end=0.1,\n","    preview_every=500\n","):\n","    \"\"\"\n","    X: np.array in [-1,1], shape (N,H,W,1)\n","    use_composites: enable C-GAN compositing after warmup.\n","    warmup_steps: train on plain reals for this many steps first.\n","    keep_plain_end: after warmup, probability to keep plain images in composite batches.\n","    \"\"\"\n","    gen = build_generator()\n","    crt = build_critic()\n","    g_opt = tf.keras.optimizers.Adam(G_LR, beta_1=0.0, beta_2=0.9)\n","    d_opt = tf.keras.optimizers.Adam(D_LR, beta_1=0.0, beta_2=0.9)\n","\n","    d_losses = []\n","    g_losses = []\n","\n","    ds_iter = make_dataset(X, batch_size=BATCH_SIZE)\n","\n","    def preview(step, n=16):\n","        z = tf.random.normal([n, LATENT_DIM])\n","        x = gen(z, training=False).numpy()\n","        x = (x + 1.0) * 0.5  # [-1,1] -> [0,1]\n","        cols = int(math.sqrt(n)); rows = math.ceil(n/cols)\n","        plt.figure(figsize=(cols*2, rows*2))\n","        for i in range(n):\n","            plt.subplot(rows, cols, i+1)\n","            plt.imshow(x[i,...,0], cmap='gray', vmin=0, vmax=1); plt.axis('off')\n","        plt.suptitle(f\"samples @ step {step}\")\n","        plt.show()\n","        if outdir:\n","            os.makedirs(outdir, exist_ok=True)\n","            for i in range(n):\n","                plt.imsave(os.path.join(outdir, f\"sample_{step:06d}_{i:02d}.png\"), x[i,...,0], cmap='gray', vmin=0, vmax=1)\n","\n","    for step in range(steps):\n","        # ----- build real batch (plain or composite) -----\n","        if use_composites and step >= warmup_steps:\n","            # ramp keep_plain from ~0.8 at warmup end -> keep_plain_end\n","            t = (step - warmup_steps) / max(1, (steps - warmup_steps))\n","            keep_plain = 0.8*(1-t) + keep_plain_end\n","            real_batch = composite_batch(X, BATCH_SIZE, k=k_comp, mode=mode, keep_plain_prob=keep_plain).astype(np.float32)\n","        else:\n","            real_batch = next(ds_iter).numpy()\n","\n","        # ----- N_CRITIC steps -----\n","        for _ in range(N_CRITIC):\n","            d_loss, wdist, gp = d_train_step(gen, crt, d_opt, real_batch)\n","\n","        # ----- 1 G step -----\n","        g_loss = g_train_step(gen, crt, g_opt, BATCH_SIZE)\n","\n","        if step % 200 == 0:\n","            print(f\"step {step:05d} | D: {d_loss.numpy():.3f} (W {wdist.numpy():.3f}, GP {gp.numpy():.3f}) | G: {g_loss.numpy():.3f} | comps={use_composites and step>=warmup_steps}\")\n","\n","        if step % preview_every == 0:\n","            preview(step)\n","\n","        d_losses.append(d_loss)\n","        g_losses.append(g_loss)\n","\n","    return gen, crt, d_losses, g_losses\n"],"metadata":{"id":"vGgPC3ikRuCg"},"id":"vGgPC3ikRuCg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading the data and run training"],"metadata":{"id":"AR7tpPMAwGiD"},"id":"AR7tpPMAwGiD"},{"cell_type":"code","source":["# Run\n","csvFile   = \"/content/drive/MyDrive/cs4243-project/preprocessing/clean/char_normalised/combined_labels_original.csv\"\n","SOURCE    = \"/content/drive/MyDrive/cs4243-project/preprocessing/clean/char_normalised/combined_images_original.npy\"\n","\n","X_train, _ = load_char_data(csvFile, SOURCE)\n","\n","# --- Simple sanity peek at real data ---\n","plt.figure(figsize=(6,6))\n","for i in range(9):\n","    idx = np.random.randint(0, len(X_train))\n","    plt.subplot(3,3,i+1)\n","    plt.imshow(((X_train[idx]+1.0)*0.5)[...,0], cmap='gray', vmin=0, vmax=1)\n","    plt.axis('off')\n","plt.suptitle(\"REAL samples\")\n","plt.show()\n","\n","# --- Train ---\n","outdir = \"/content/drive/MyDrive/cs4243-project/outputs/cgan_samples_3\"\n","generator, critic, d_loss, g_loss = train(\n","    X_train,\n","    steps=10000,\n","    outdir=outdir,\n","    use_composites=True,   # turn on C-GAN compositing\n","    k_comp=2,              # compose 2 sources (like paper)\n","    mode=\"mask\",           # pixel replacement\n","    warmup_steps=8000,     # learn plain manifold first\n","    keep_plain_end=0.10,   # still keep some plain reals later\n","    preview_every=500\n",")\n","\n","# --- Save generator ---\n","generator.save(os.path.join(outdir, \"generator_wgangp_cgan_3.h5\"))\n","print(f\"Saved generator to {outdir}\")"],"metadata":{"id":"44haQUV-ywAq","collapsed":true},"id":"44haQUV-ywAq","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Plot generator and critic loss"],"metadata":{"id":"SbEx35J-wRhp"},"id":"SbEx35J-wRhp"},{"cell_type":"code","metadata":{"id":"bf54ee5b"},"source":["g_loss_values = [g.numpy() for g in g_loss]\n","d_loss_values = [d.numpy() for d in d_loss]\n","\n","plt.figure(figsize=(10, 6))\n","plt.plot(g_loss_values, label='Generator Loss')\n","plt.plot(d_loss_values, label='Discriminator Loss')\n","plt.title('Generator and Discriminator Loss Over Training Steps')\n","plt.xlabel('Training Step')\n","plt.ylabel('Loss Value')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"id":"bf54ee5b","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generating characters"],"metadata":{"id":"6QEgY4tBwVOr"},"id":"6QEgY4tBwVOr"},{"cell_type":"code","source":["# loading of the model, had 3 different models with different hyperparameter tuning\n","# model_path = \"/content/drive/MyDrive/cs4243-project/outputs/cgan_samples/generator_wgangp_cgan.h5\"\n","# model_path = \"/content/drive/MyDrive/cs4243-project/outputs/cgan_samples_2/generator_wgangp_cgan_2.h5\"\n","model_path = \"/content/drive/MyDrive/cs4243-project/outputs/cgan_samples_3/generator_wgangp_cgan_3.h5\"\n","gen = load_model(model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyBBvVuIc75A","executionInfo":{"status":"ok","timestamp":1763047641338,"user_tz":-480,"elapsed":204,"user":{"displayName":"Bryan Ho","userId":"17111691639907127460"}},"outputId":"73c9c07f-452a-4f9a-d5f7-9e898b0353e0","collapsed":true},"id":"oyBBvVuIc75A","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}]},{"cell_type":"code","source":["LATENT_DIM = 100  # same as training\n","\n","# --- Generate random latent vectors ---\n","n = 16   # number of images to generate\n","z = np.random.normal(0, 1, (n, LATENT_DIM))\n","generated = gen.predict(z, verbose=0)   # shape (n, 80, 80, 1), range [-1,1]\n","generated = (generated + 1.0) * 0.5     # map to [0,1] for viewing\n","\n","\n","# --- Preview as grid ---\n","cols = int(np.sqrt(n)); rows = int(np.ceil(n/cols))\n","plt.figure(figsize=(cols*2, rows*2))\n","for i in range(n):\n","    plt.subplot(rows, cols, i+1)\n","    plt.imshow(final_images[i,...,0], cmap='gray', vmin=0, vmax=1)\n","    plt.axis('off')\n","plt.suptitle(\"Generated characters\")\n","plt.show()"],"metadata":{"id":"LtPfD6g6aHXE"},"id":"LtPfD6g6aHXE","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45ee165c"},"source":["### Saving Generated Characters"],"id":"45ee165c"},{"cell_type":"code","metadata":{"id":"62a36e03"},"source":["# Save generated characters to be used for training and testing\n","save_dir = \"/content/drive/MyDrive/cs4243-project/outputs/generated_characters\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","print(f\"Saving {len(final_images)} generated characters to {save_dir}\")\n","\n","for i, img_array in enumerate(final_images):\n","    # Ensure the image array is 2D for grayscale saving\n","    if img_array.ndim == 3 and img_array.shape[2] == 1:\n","        img_array = img_array[..., 0]\n","\n","    file_path = os.path.join(save_dir, f\"generated_char_{i:03d}.png\")\n","    plt.imsave(file_path, img_array, cmap='gray', vmin=0, vmax=1)\n","\n","print(\"Generated characters saved successfully.\")"],"id":"62a36e03","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluations\n","\n","This evaluation aims to compare the quality of several WGAN-based generators by using both perceptual and distribution-level metrics. We generate a large set of samples from each model and compare them against real character images using **FID**, which measures how close the generated image distribution is to the real one, and **SSIM/PSNR**, which quantify visual similarity to the nearest real sample.\n","\n","These metrics help reveal whether a model produces sharp, realistic characters, avoids mode collapse, and captures the variability present in real data. Together, they provide a structured and objective way to determine which generator produces the most realistic and useful CAPTCHA characters."],"metadata":{"id":"yxYonPCjEu5L"},"id":"yxYonPCjEu5L"},{"cell_type":"code","source":["# Evaluation script for multiple WGAN generators\n","import os\n","import numpy as np\n","from glob import glob\n","from tqdm import tqdm\n","import tensorflow as tf\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input as inception_preprocess\n","from scipy.linalg import sqrtm\n","from skimage.metrics import structural_similarity as ssim\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from PIL import Image\n","\n","# ---------- PARAMETERS ----------\n","MODEL_PATHS = [\n","    \"/content/drive/MyDrive/cs4243-project/outputs/cgan_samples/generator_wgangp_cgan.h5\",\n","    \"/content/drive/MyDrive/cs4243-project/outputs/cgan_samples_2/generator_wgangp_cgan_2.h5\",\n","    \"/content/drive/MyDrive/cs4243-project/outputs/cgan_samples_3/generator_wgangp_cgan_3.h5\",\n","]\n","LATENT_DIM = 100\n","N_SAMPLES = 2048  # number of generated samples to use for FID/IS\n","BATCH = 64\n","REAL_DIR = '/content/drive/MyDrive/cs4243-project/preprocessing/clean/char_normalised/combined_labels_original.csv'\n","REAL_NPY = '/content/drive/MyDrive/cs4243-project/preprocessing/clean/char_normalised/combined_images_original.npy'\n","IMAGE_SHAPE = (80, 80)  # your generator output shape (H,W)\n","RANDOM_SEED = 42\n","CLASSIFIER_PATH = '/content/drive/MyDrive/cs4243-project/models/classfier_test.h5'\n","\n","# ---------- UTILITIES ----------\n","np.random.seed(RANDOM_SEED)\n","tf.random.set_seed(RANDOM_SEED)\n","\n","def load_real_images_from_dir(dirname, target_shape=(80,80), max_images=None):\n","    paths = sorted(glob(os.path.join(dirname, \"*.*\")))\n","    if max_images:\n","        paths = paths[:max_images]\n","    imgs = []\n","    for p in paths:\n","        try:\n","            im = Image.open(p).convert('L').resize(target_shape, Image.BILINEAR)\n","            arr = np.asarray(im, dtype=np.float32) / 255.0  # [0,1]\n","            if arr.ndim == 2:\n","                arr = arr[..., np.newaxis]\n","            imgs.append(arr)\n","        except Exception as e:\n","            print(\"Skipping\", p, \":\", e)\n","    imgs = np.stack(imgs, axis=0)\n","    return imgs\n","\n","def load_real_images(npy_path=None, dir_path=None, target_shape=(80,80), max_images=None):\n","    if npy_path:\n","        data = np.load(npy_path)\n","        # Expect shape (N,H,W) or (N,H,W,1) or (N,H,W,3)\n","        if data.ndim == 3:\n","            data = data[..., np.newaxis]\n","        # normalize if necessary\n","        if data.max() > 2.0:\n","            data = data.astype(np.float32) / 255.0\n","        if target_shape is not None and (data.shape[1], data.shape[2]) != target_shape:\n","            # resize\n","            resized = []\n","            for im in data:\n","                p = Image.fromarray((im.squeeze()*255).astype(np.uint8)).resize(target_shape, Image.BILINEAR)\n","                a = np.asarray(p, dtype=np.float32)/255.0\n","                if a.ndim==2: a = a[..., np.newaxis]\n","                resized.append(a)\n","            data = np.stack(resized, axis=0)\n","        if max_images:\n","            data = data[:max_images]\n","        return data\n","    elif dir_path:\n","        return load_real_images_from_dir(dir_path, target_shape=target_shape, max_images=max_images)\n","    else:\n","        raise ValueError(\"Provide npy_path or dir_path\")\n","\n","def to_3ch_and_resize(images, size=(299,299)):\n","    # images: (N,H,W,1) values in [0,1]\n","    N = images.shape[0]\n","    out = np.zeros((N, size[0], size[1], 3), dtype=np.float32)\n","    for i in range(N):\n","        im = images[i,...,0]\n","        pil = Image.fromarray((im*255).astype(np.uint8)).resize((size[1], size[0]), Image.BILINEAR)\n","        arr = np.asarray(pil, dtype=np.float32) / 255.0\n","        if arr.ndim == 2:\n","            arr = np.stack([arr,arr,arr], axis=-1)\n","        out[i] = arr\n","    return out\n","\n","# ---------- FID / IS helpers ----------\n","def get_inception_activations(images, model, batch_size=32):\n","    # images assumed in [0,1], shape (N,H,W,3); model expects preprocessed inputs\n","    N = images.shape[0]\n","    acts = []\n","    for i in range(0, N, batch_size):\n","        batch = images[i:i+batch_size]\n","        x = inception_preprocess(batch*255.0)  # Inception preprocess expects pixels in range [-1,1]\n","        act = model.predict(x, verbose=0)\n","        acts.append(act)\n","    acts = np.vstack(acts)\n","    return acts\n","\n","def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n","    \"\"\"Numpy implementation of FID formula\"\"\"\n","    diff = mu1 - mu2\n","    covmean = sqrtm(sigma1.dot(sigma2))\n","    # numerical error might give slight imaginary component\n","    if np.iscomplexobj(covmean):\n","        covmean = covmean.real\n","    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2*covmean)\n","    return np.real(fid)\n","\n","def compute_fid(real_images, gen_images, inception_model):\n","    # both inputs should be (N,H,W,1) in [0,1]\n","    real_3 = to_3ch_and_resize(real_images, size=(299,299))\n","    gen_3 = to_3ch_and_resize(gen_images, size=(299,299))\n","    act_real = get_inception_activations(real_3, inception_model, batch_size=BATCH)\n","    act_gen  = get_inception_activations(gen_3, inception_model, batch_size=BATCH)\n","    mu1 = np.mean(act_real, axis=0)\n","    mu2 = np.mean(act_gen, axis=0)\n","    sigma1 = np.cov(act_real, rowvar=False)\n","    sigma2 = np.cov(act_gen, rowvar=False)\n","    fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n","    return fid\n","\n","# ---------- SSIM / PSNR nearest-neighbor ----------\n","def compute_nearest_ssim_psnr(real_images, gen_images, sample_n=1024):\n","    # For each generated image, find nearest real by L2 in pixel space (fast brute force)\n","    Ngen = min(len(gen_images), sample_n)\n","    Nreal = len(real_images)\n","    gens = gen_images[:Ngen]\n","    reals = real_images\n","    ssim_vals = []\n","    psnr_vals = []\n","    # flatten reals\n","    reals_flat = reals.reshape((Nreal, -1))\n","    for g in gens:\n","        g_flat = g.reshape(-1)\n","        # find nearest by Euclidean\n","        dists = np.sum((reals_flat - g_flat)**2, axis=1)\n","        idx = np.argmin(dists)\n","        r = reals[idx,...,0]\n","        ssim_v = ssim((r*255).astype(np.uint8), (g[...,0]*255).astype(np.uint8), data_range=255)\n","        psnr_v = psnr((r*255).astype(np.uint8), (g[...,0]*255).astype(np.uint8), data_range=255)\n","        ssim_vals.append(ssim_v)\n","        psnr_vals.append(psnr_v)\n","    return np.mean(ssim_vals), np.std(ssim_vals), np.mean(psnr_vals), np.std(psnr_vals)\n","\n","# ---------- Generate samples with a generator ----------\n","def generate_images_from_generator(gen, n_samples, latent_dim, batch_size=64):\n","    out = []\n","    steps = int(np.ceil(n_samples / batch_size))\n","    for _ in range(steps):\n","        z = np.random.normal(0, 1, (batch_size, latent_dim))\n","        g = gen.predict(z, verbose=0)\n","        # map from [-1,1] -> [0,1] if necessary\n","        if g.min() < -0.5:\n","            g = (g + 1.0) * 0.5\n","        # ensure shape (batch,H,W,1)\n","        if g.ndim == 3:\n","            g = g[..., np.newaxis]\n","        out.append(g)\n","    out = np.vstack(out)[:n_samples]\n","    return out.astype(np.float32)\n","\n","# ---------- Optional classifier evaluation ----------\n","def evaluate_with_classifier(classifier_model, gen_images, label_map=None):\n","    # classifier expects same preproc as used in training; here we assume it accepts (N,H,W,1) with [0,1]\n","    preds = classifier_model.predict(gen_images, verbose=0)\n","    pred_labels = np.argmax(preds, axis=1)\n","    confidences = np.max(preds, axis=1)\n","    # if label_map exists you can map indices -> chars. Return distribution & mean confidence.\n","    return pred_labels, confidences\n","\n","# ---------- MAIN: run evaluation on list of models ----------\n","def evaluate_models(model_paths, real_images, latent_dim=100, n_samples=2048):\n","    # Prepare Inception model for feature extraction\n","    inception = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n","    results = []\n","    for path in model_paths:\n","        print(\"Loading:\", path)\n","        gen = load_model(path, compile=False)\n","        # generate images\n","        gen_images = generate_images_from_generator(gen, n_samples, latent_dim, batch_size=BATCH)\n","        print(f\"Generated {len(gen_images)} images from {os.path.basename(path)}\")\n","        # sample same number of real images (random)\n","        if len(real_images) < n_samples:\n","            idx = np.random.choice(len(real_images), n_samples, replace=True)\n","        else:\n","            idx = np.random.choice(len(real_images), n_samples, replace=False)\n","        real_sample = real_images[idx]\n","        # FID\n","        fid_val = compute_fid(real_sample, gen_images, inception)\n","        # SSIM / PSNR nearest neighbor (sample subset)\n","        ssim_mean, ssim_std, psnr_mean, psnr_std = compute_nearest_ssim_psnr(real_images, gen_images, sample_n=512)\n","        res = {\n","            \"model_path\": path,\n","            \"fid\": float(fid_val),\n","            \"ssim_mean\": float(ssim_mean),\n","            \"ssim_std\": float(ssim_std),\n","            \"psnr_mean\": float(psnr_mean),\n","            \"psnr_std\": float(psnr_std),\n","            \"n_generated\": len(gen_images)\n","        }\n","        results.append(res)\n","        print(\" -> FID:\", res[\"fid\"])\n","        print(f\" -> SSIM mean: {res['ssim_mean']:.4f} ± {res['ssim_std']:.4f}\")\n","        print(f\" -> PSNR mean: {res['psnr_mean']:.2f} ± {res['psnr_std']:.2f}\")\n","        print(\"--------------------------------------------------\")\n","    return results\n","\n","if __name__ == \"__main__\":\n","    # Load real dataset\n","    if REAL_NPY:\n","        real_images = load_real_images(npy_path=REAL_NPY, target_shape=IMAGE_SHAPE)\n","    else:\n","        real_images = load_real_images(dir_path=REAL_DIR, target_shape=IMAGE_SHAPE, max_images=10000)\n","    print(\"Real images loaded:\", real_images.shape)\n","    metrics = evaluate_models(MODEL_PATHS, real_images, latent_dim=LATENT_DIM, n_samples=N_SAMPLES)\n","\n","    # optional: classifier evaluation (if CLASSIFIER_PATH provided)\n","    if CLASSIFIER_PATH:\n","        print(\"Loading classifier:\", CLASSIFIER_PATH)\n","        clf = load_model(CLASSIFIER_PATH, compile=False)\n","        for m in MODEL_PATHS:\n","            gen = load_model(m, compile=False)\n","            gen_samples = generate_images_from_generator(gen, 1024, LATENT_DIM, batch_size=BATCH)\n","            preds, confs = evaluate_with_classifier(clf, gen_samples)\n","            acc = np.mean(preds == preds)  # replace with true label comparison if you have target labels\n","            print(f\"{os.path.basename(m)} classifier mean confidence: {confs.mean():.4f}\")\n","\n","    # Print summary table\n","    import pandas as pd\n","    df = pd.DataFrame(metrics)\n","    df = df.sort_values(\"fid\")\n","    print(\"\\nSummary (lower FID = better):\")\n","    display(df)\n"],"metadata":{"id":"ZrvSAWrMEwhM"},"id":"ZrvSAWrMEwhM","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}